# Machine Learning Projects

Each branch corresponds to the following project description:

**Project 1:** This project aims to train a supervised Machine Learning algorithm with 10 perceptrons over a span of 153 epochs distributed equally between 3 different learning rates. The goal by the end of the training is for the algorithm to identify handwritten digits accurately. The process will be done on two datasets, the first dataset is the training set containing 60000 images for the algorithm to be trained on, and the second is the testing set containing 10000 images for the algorithm to be tested on to ensure that there is no occurrence of the overfitting problem.

**Project 2:** This project aims to train a supervised Machine Learning neural network over two different
experiments. For **experiment #1**, the entire training dataset will be used over three different neural
networks with 20, 50, and 100 hidden units respectively. For **experiment #2**, two neural networks
are created with a fixed number of hidden units (100). However, the two neural networks will be using respectively one-quarter and one-half of the training dataset. In both experiments, the neural networks
will execute techniques such as forward and back propagation to update the weights of its respective network.
After training the neural networks throughout 51 epochs each, accuracy percentages will be
computed by presenting the same training dataset and a test dataset to the networks in-order to
computed prediction correctness over the dataset for the different networks.

**Project 3:** This project aims to train a supervised Machine Learning Naive Bayes classifier to predict if a given set of features belong to one of a set of classes P(feature | Class). Due to the simplicity of Naive Bayes classifiers, the accuracy of Naive Bayes classifiers is notably lower in comparison to previous projects.

**Project 4**: This project aims to train an unsupervised Machine Learning K-Means Clustering algorithm to classify similar handwritten digits into distinct subgroups (clusters). It starts with selecting 10 random cluster centers and uses those cluster centers to join all inputs to the cluster closest to the respective input. After every iteration, the cluster centers are recomputed by computing the means of obtained clusters. The process comes to an end when the set of cluster centers does not change in comparison to the previous iteration.
